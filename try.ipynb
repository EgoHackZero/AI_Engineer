{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281d02b5",
   "metadata": {},
   "source": [
    "# First usecase just to make sure everything is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174f4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is a city rich in history, culture, and beauty. Here are some must-see attractions and experiences you shouldn't miss:\n",
      "\n",
      "1. **Eiffel Tower**: An iconic symbol of Paris, you can either admire it from the ground or take a lift to the top for a breathtaking view of the city.\n",
      "\n",
      "2. **Louvre Museum**: Home to thousands of works of art, including the Mona Lisa and the Winged Victory. The museum is housed in the Louvre Palace, a former royal residence.\n",
      "\n",
      "3. **Notre-Dame Cathedral**: Although severely damaged by a fire in 2019, the cathedral remains an important historical and architectural landmark. Restoration work is ongoing.\n",
      "\n",
      "4. **Montmartre**: Known for its bohemian past, charming streets, and the stunning Sacré-Cœur Basilica at its summit. Don't miss Place du Tertre, where local artists create and sell their work.\n",
      "\n",
      "5. **Champs-Élysées and Arc de Triomphe**: Stroll down the famous avenue, lined with luxury shops and cafes, and visit the iconic Arc de Triomphe at its end.\n",
      "\n",
      "6. **Seine River Cruise**: A relaxing way to see many of Paris's most famous landmarks, such as the Eiffel Tower, Notre-Dame, and the Louvre.\n",
      "\n",
      "7. **Musée d'Orsay**: Housed in a former railway station, this museum is home to an impressive collection of Impressionist and Post-Impressionist masterpieces.\n",
      "\n",
      "8. **Palais Garnier**: The famous opera house that inspired the novel \"The Phantom of the Opera.\" You can take a guided tour of the opulent building.\n",
      "\n",
      "9. **Luxembourg Garden**: A beautiful public park where you can relax, enjoy a picnic, or visit the Luxembourg Palace.\n",
      "\n",
      "10. **Marais District**: Known for its trendy boutiques, art galleries, and vibrant nightlife. Don't miss the historic Place des Vosges.\n",
      "\n",
      "11. **Sainte-Chapelle**: A stunning Gothic chapel known for its incredible stained-glass windows.\n",
      "\n",
      "12. **Versailles**: If you have time for a day trip, visit the Palace of Versailles and its beautiful gardens, a short train ride from Paris.\n",
      "\n",
      "13. **French Cuisine**: Enjoy Parisian cafes, bistros, and patisseries. Try local specialties like croissants, baguettes, escargot, and coq au vin.\n",
      "\n",
      "14. **Cabaret Shows**: Experience the glamour of Paris with a show at the Moulin Rouge or other famous cabarets.\n",
      "\n",
      "Enjoy your trip to Paris! It's a city full of enchantment and romance.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_MISTRAL_API_ENDPOINT\")\n",
    "model_name = \"mistral-small-2503\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_API_KEY\")),\n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=\"I am going to Paris, what should I see?\"),\n",
    "    ],\n",
    "    max_tokens=2048,\n",
    "    temperature=0.8,\n",
    "    top_p=0.1,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93357a30",
   "metadata": {},
   "source": [
    "# How to analyze an image\n",
    "\n",
    "Note the following requirements for image analysis:\n",
    "\n",
    "- The image must be presented in JPEG, PNG, GIF, or BMP format.\n",
    "- The file size of the image must be less than 4 megabytes (MB).\n",
    "- The dimensions of the image must be greater than 50 x 50 pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55275a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal\n",
      "mammal\n",
      "pet\n",
      "dog\n",
      "outdoor\n",
      "dog breed\n",
      "grass\n",
      "snout\n",
      "corgi\n",
      "ground\n",
      "brown\n",
      "looking\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=os.getenv(\"AZURE_CV4_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_CV4_KEY\"))\n",
    ")\n",
    "\n",
    "result = client.analyze_from_url(\n",
    "    image_url=\"https://www.zooplus.fr/magazine/wp-content/uploads/2023/05/corgi-1.jpeg\", # Binary data from your image file\n",
    "    visual_features=[VisualFeatures.TAGS]\n",
    ")\n",
    "\n",
    "for tag in result[\"tagsResult\"][\"values\"]:\n",
    "    print(tag[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de027cf3",
   "metadata": {},
   "source": [
    "## Exercise - Analyze images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5eafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "import requests\n",
    "\n",
    "# import namespaces\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Clear the console\n",
    "    os.system('cls' if os.name=='nt' else 'clear')\n",
    "\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        ai_endpoint = os.getenv('AZURE_CV4_ENDPOINT')\n",
    "        ai_key = os.getenv('AZURE_CV4_KEY')\n",
    "\n",
    "        # Get image\n",
    "        image_file = 'images/street.jpg'\n",
    "        if len(sys.argv) > 1:\n",
    "            image_file = sys.argv[1]\n",
    "        \n",
    "\n",
    "        # Authenticate Azure AI Vision client\n",
    "        cv_client = ImageAnalysisClient(\n",
    "            endpoint=ai_endpoint,\n",
    "            credential=AzureKeyCredential(ai_key))\n",
    "\n",
    "\n",
    "        # Analyze image\n",
    "        with open(image_file, \"rb\") as f:\n",
    "            image_data = f.read()\n",
    "        print(f'\\nAnalyzing {image_file}\\n')\n",
    "\n",
    "        result = cv_client.analyze(\n",
    "            image_data=image_data,\n",
    "            visual_features=[\n",
    "                VisualFeatures.CAPTION,\n",
    "                VisualFeatures.DENSE_CAPTIONS,\n",
    "                VisualFeatures.TAGS,\n",
    "                VisualFeatures.OBJECTS,\n",
    "                VisualFeatures.PEOPLE],\n",
    "        )\n",
    "\n",
    "\n",
    "        # Get image captions\n",
    "        if result.caption is not None:\n",
    "            print(\"\\nCaption:\")\n",
    "            print(\" Caption: '{}' (confidence: {:.2f}%)\".format(result.caption.text, result.caption.confidence * 100))\n",
    "            \n",
    "        if result.dense_captions is not None:\n",
    "            print(\"\\nDense Captions:\")\n",
    "            for caption in result.dense_captions.list:\n",
    "                print(\" Caption: '{}' (confidence: {:.2f}%)\".format(caption.text, caption.confidence * 100))\n",
    "        \n",
    "\n",
    "        # Get image tags\n",
    "        if result.tags is not None:\n",
    "            print(\"\\nTags:\")\n",
    "            for tag in result.tags.list:\n",
    "                print(\" Tag: '{}' (confidence: {:.2f}%)\".format(tag.name, tag.confidence * 100))\n",
    "\n",
    "\n",
    "        # Get objects in the image\n",
    "        if result.objects is not None:\n",
    "            print(\"\\nObjects in image:\")\n",
    "            for detected_object in result.objects.list:\n",
    "                # Print object tag and confidence\n",
    "                print(\" {} (confidence: {:.2f}%)\".format(detected_object.tags[0].name, detected_object.tags[0].confidence * 100))\n",
    "            # Annotate objects in the image\n",
    "            show_objects(image_file, result.objects.list)\n",
    "\n",
    "\n",
    "        # Get people in the image\n",
    "        if result.people is not None:\n",
    "            print(\"\\nPeople in image:\")\n",
    "\n",
    "            for detected_person in result.people.list:\n",
    "                if detected_person.confidence > 0.2:\n",
    "                    # Print location and confidence of each person detected\n",
    "                    print(\" {} (confidence: {:.2f}%)\".format(detected_person.bounding_box, detected_person.confidence * 100))\n",
    "            # Annotate people in the image\n",
    "            show_people(image_file, result.people.list)\n",
    "  \n",
    "            \n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "\n",
    "def show_objects(image_filename, detected_objects):\n",
    "    print (\"\\nAnnotating objects...\")\n",
    "\n",
    "    # Prepare image for drawing\n",
    "    image = Image.open(image_filename)\n",
    "    fig = plt.figure(figsize=(image.width/100, image.height/100))\n",
    "    plt.axis('off')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color = 'cyan'\n",
    "\n",
    "    for detected_object in detected_objects:\n",
    "        # Draw object bounding box\n",
    "        r = detected_object.bounding_box\n",
    "        bounding_box = ((r.x, r.y), (r.x + r.width, r.y + r.height)) \n",
    "        draw.rectangle(bounding_box, outline=color, width=3)\n",
    "        plt.annotate(detected_object.tags[0].name,(r.x, r.y), backgroundcolor=color)\n",
    "\n",
    "    # Save annotated image\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout(pad=0)\n",
    "    objectfile = 'objects.jpg'\n",
    "    fig.savefig(objectfile)\n",
    "    print('  Results saved in', objectfile)\n",
    "\n",
    "\n",
    "def show_people(image_filename, detected_people):\n",
    "    print (\"\\nAnnotating objects...\")\n",
    "\n",
    "    # Prepare image for drawing\n",
    "    image = Image.open(image_filename)\n",
    "    fig = plt.figure(figsize=(image.width/100, image.height/100))\n",
    "    plt.axis('off')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color = 'cyan'\n",
    "\n",
    "    for detected_person in detected_people:\n",
    "        if detected_person.confidence > 0.2:\n",
    "            # Draw object bounding box\n",
    "            r = detected_person.bounding_box\n",
    "            bounding_box = ((r.x, r.y), (r.x + r.width, r.y + r.height))\n",
    "            draw.rectangle(bounding_box, outline=color, width=3)\n",
    "\n",
    "    # Save annotated image\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout(pad=0)\n",
    "    peoplefile = 'people.jpg'\n",
    "    fig.savefig(peoplefile)\n",
    "    print('  Results saved in', peoplefile)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c6801",
   "metadata": {},
   "source": [
    "# Img-to-text (optical character recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a72569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'modelVersion': '2023-10-01', 'metadata': {'width': 2000, 'height': 2000}, 'readResult': {'blocks': [{'lines': [{'text': '# OF SERVINGS', 'boundingPolygon': [{'x': 858, 'y': 258}, {'x': 1005, 'y': 232}, {'x': 1008, 'y': 257}, {'x': 862, 'y': 282}], 'words': [{'text': '#', 'boundingPolygon': [{'x': 858, 'y': 260}, {'x': 872, 'y': 257}, {'x': 877, 'y': 280}, {'x': 863, 'y': 282}], 'confidence': 0.991}, {'text': 'OF', 'boundingPolygon': [{'x': 879, 'y': 256}, {'x': 905, 'y': 250}, {'x': 909, 'y': 274}, {'x': 883, 'y': 278}], 'confidence': 0.997}, {'text': 'SERVINGS', 'boundingPolygon': [{'x': 910, 'y': 249}, {'x': 1005, 'y': 232}, {'x': 1007, 'y': 259}, {'x': 914, 'y': 273}], 'confidence': 0.993}]}, {'text': 'TOTAL TIME', 'boundingPolygon': [{'x': 717, 'y': 280}, {'x': 827, 'y': 267}, {'x': 830, 'y': 287}, {'x': 721, 'y': 304}], 'words': [{'text': 'TOTAL', 'boundingPolygon': [{'x': 719, 'y': 281}, {'x': 776, 'y': 274}, {'x': 779, 'y': 296}, {'x': 722, 'y': 304}], 'confidence': 0.994}, {'text': 'TIME', 'boundingPolygon': [{'x': 782, 'y': 273}, {'x': 827, 'y': 267}, {'x': 828, 'y': 285}, {'x': 785, 'y': 294}], 'confidence': 0.988}]}, {'text': 'Recipe Check', 'boundingPolygon': [{'x': 420, 'y': 208}, {'x': 942, 'y': 126}, {'x': 953, 'y': 220}, {'x': 433, 'y': 304}], 'words': [{'text': 'Recipe', 'boundingPolygon': [{'x': 421, 'y': 208}, {'x': 686, 'y': 166}, {'x': 698, 'y': 264}, {'x': 434, 'y': 304}], 'confidence': 0.959}, {'text': 'Check', 'boundingPolygon': [{'x': 705, 'y': 163}, {'x': 925, 'y': 129}, {'x': 935, 'y': 225}, {'x': 717, 'y': 261}], 'confidence': 0.863}]}, {'text': 'COOK TIME', 'boundingPolygon': [{'x': 557, 'y': 305}, {'x': 675, 'y': 289}, {'x': 678, 'y': 312}, {'x': 560, 'y': 328}], 'words': [{'text': 'COOK', 'boundingPolygon': [{'x': 559, 'y': 305}, {'x': 613, 'y': 299}, {'x': 618, 'y': 321}, {'x': 564, 'y': 328}], 'confidence': 0.989}, {'text': 'TIME', 'boundingPolygon': [{'x': 621, 'y': 298}, {'x': 664, 'y': 291}, {'x': 669, 'y': 313}, {'x': 626, 'y': 319}], 'confidence': 0.984}]}, {'text': 'AMCLINT', 'boundingPolygon': [{'x': 922, 'y': 462}, {'x': 1002, 'y': 450}, {'x': 1005, 'y': 470}, {'x': 924, 'y': 483}], 'words': [{'text': 'AMCLINT', 'boundingPolygon': [{'x': 924, 'y': 462}, {'x': 1002, 'y': 451}, {'x': 1004, 'y': 471}, {'x': 926, 'y': 483}], 'confidence': 0.599}]}, {'text': 'PREP TIME', 'boundingPolygon': [{'x': 400, 'y': 327}, {'x': 507, 'y': 312}, {'x': 512, 'y': 342}, {'x': 405, 'y': 359}], 'words': [{'text': 'PREP', 'boundingPolygon': [{'x': 404, 'y': 327}, {'x': 451, 'y': 321}, {'x': 456, 'y': 352}, {'x': 409, 'y': 359}], 'confidence': 0.989}, {'text': 'TIME', 'boundingPolygon': [{'x': 457, 'y': 320}, {'x': 504, 'y': 313}, {'x': 509, 'y': 342}, {'x': 462, 'y': 351}], 'confidence': 0.983}]}, {'text': 'Recipe Check', 'boundingPolygon': [{'x': 1134, 'y': 594}, {'x': 1654, 'y': 745}, {'x': 1630, 'y': 823}, {'x': 1112, 'y': 674}], 'words': [{'text': 'Recipe', 'boundingPolygon': [{'x': 1138, 'y': 595}, {'x': 1388, 'y': 670}, {'x': 1360, 'y': 749}, {'x': 1112, 'y': 671}], 'confidence': 0.994}, {'text': 'Check', 'boundingPolygon': [{'x': 1415, 'y': 678}, {'x': 1639, 'y': 742}, {'x': 1611, 'y': 820}, {'x': 1387, 'y': 757}], 'confidence': 0.997}]}, {'text': 'APPETIZER - BREAKFAST - LUNCH - DINNER - DESSERT - SNACK', 'boundingPolygon': [{'x': 417, 'y': 430}, {'x': 1006, 'y': 335}, {'x': 1009, 'y': 361}, {'x': 421, 'y': 456}], 'words': [{'text': 'APPETIZER', 'boundingPolygon': [{'x': 418, 'y': 432}, {'x': 512, 'y': 417}, {'x': 515, 'y': 440}, {'x': 421, 'y': 456}], 'confidence': 0.993}, {'text': '-', 'boundingPolygon': [{'x': 517, 'y': 416}, {'x': 528, 'y': 414}, {'x': 531, 'y': 438}, {'x': 520, 'y': 440}], 'confidence': 0.976}, {'text': 'BREAKFAST', 'boundingPolygon': [{'x': 533, 'y': 413}, {'x': 638, 'y': 396}, {'x': 641, 'y': 420}, {'x': 536, 'y': 437}], 'confidence': 0.993}, {'text': '-', 'boundingPolygon': [{'x': 643, 'y': 395}, {'x': 654, 'y': 394}, {'x': 657, 'y': 417}, {'x': 645, 'y': 419}], 'confidence': 0.959}, {'text': 'LUNCH', 'boundingPolygon': [{'x': 659, 'y': 393}, {'x': 721, 'y': 383}, {'x': 723, 'y': 407}, {'x': 661, 'y': 416}], 'confidence': 0.995}, {'text': '-', 'boundingPolygon': [{'x': 727, 'y': 381}, {'x': 740, 'y': 379}, {'x': 742, 'y': 404}, {'x': 730, 'y': 406}], 'confidence': 0.986}, {'text': 'DINNER', 'boundingPolygon': [{'x': 745, 'y': 379}, {'x': 815, 'y': 367}, {'x': 817, 'y': 392}, {'x': 747, 'y': 403}], 'confidence': 0.994}, {'text': '-', 'boundingPolygon': [{'x': 820, 'y': 366}, {'x': 831, 'y': 364}, {'x': 833, 'y': 389}, {'x': 822, 'y': 391}], 'confidence': 0.993}, {'text': 'DESSERT', 'boundingPolygon': [{'x': 836, 'y': 363}, {'x': 917, 'y': 350}, {'x': 919, 'y': 376}, {'x': 838, 'y': 388}], 'confidence': 0.994}, {'text': '-', 'boundingPolygon': [{'x': 922, 'y': 349}, {'x': 934, 'y': 347}, {'x': 936, 'y': 373}, {'x': 924, 'y': 375}], 'confidence': 0.99}, {'text': 'SNACK', 'boundingPolygon': [{'x': 939, 'y': 346}, {'x': 1003, 'y': 336}, {'x': 1004, 'y': 363}, {'x': 941, 'y': 372}], 'confidence': 0.993}]}, {'text': 'INGREDIENTS', 'boundingPolygon': [{'x': 602, 'y': 511}, {'x': 722, 'y': 494}, {'x': 725, 'y': 514}, {'x': 605, 'y': 533}], 'words': [{'text': 'INGREDIENTS', 'boundingPolygon': [{'x': 603, 'y': 512}, {'x': 723, 'y': 494}, {'x': 725, 'y': 514}, {'x': 607, 'y': 534}], 'confidence': 0.99}]}, {'text': 'RECIPE', 'boundingPolygon': [{'x': 402, 'y': 493}, {'x': 465, 'y': 486}, {'x': 468, 'y': 505}, {'x': 404, 'y': 514}], 'words': [{'text': 'RECIPE', 'boundingPolygon': [{'x': 403, 'y': 494}, {'x': 465, 'y': 487}, {'x': 466, 'y': 503}, {'x': 404, 'y': 514}], 'confidence': 0.992}]}, {'text': 'PREP TIME', 'boundingPolygon': [{'x': 1083, 'y': 683}, {'x': 1203, 'y': 722}, {'x': 1194, 'y': 749}, {'x': 1074, 'y': 712}], 'words': [{'text': 'PREP', 'boundingPolygon': [{'x': 1085, 'y': 684}, {'x': 1130, 'y': 701}, {'x': 1119, 'y': 727}, {'x': 1075, 'y': 709}], 'confidence': 0.989}, {'text': 'TIME', 'boundingPolygon': [{'x': 1136, 'y': 703}, {'x': 1182, 'y': 717}, {'x': 1169, 'y': 744}, {'x': 1124, 'y': 729}], 'confidence': 0.987}]}, {'text': 'COOK TIME', 'boundingPolygon': [{'x': 1229, 'y': 731}, {'x': 1337, 'y': 769}, {'x': 1329, 'y': 791}, {'x': 1221, 'y': 751}], 'words': [{'text': 'COOK', 'boundingPolygon': [{'x': 1235, 'y': 733}, {'x': 1286, 'y': 750}, {'x': 1279, 'y': 771}, {'x': 1227, 'y': 754}], 'confidence': 0.991}, {'text': 'TIME', 'boundingPolygon': [{'x': 1294, 'y': 754}, {'x': 1335, 'y': 771}, {'x': 1328, 'y': 791}, {'x': 1287, 'y': 774}], 'confidence': 0.985}]}, {'text': 'TOTAL TIME', 'boundingPolygon': [{'x': 1383, 'y': 781}, {'x': 1507, 'y': 821}, {'x': 1499, 'y': 843}, {'x': 1378, 'y': 801}], 'words': [{'text': 'TOTAL', 'boundingPolygon': [{'x': 1388, 'y': 782}, {'x': 1444, 'y': 800}, {'x': 1438, 'y': 823}, {'x': 1383, 'y': 804}], 'confidence': 0.993}, {'text': 'TIME', 'boundingPolygon': [{'x': 1450, 'y': 802}, {'x': 1493, 'y': 817}, {'x': 1487, 'y': 839}, {'x': 1444, 'y': 825}], 'confidence': 0.989}]}, {'text': '# OF SERVINGS', 'boundingPolygon': [{'x': 1513, 'y': 816}, {'x': 1672, 'y': 870}, {'x': 1665, 'y': 892}, {'x': 1506, 'y': 840}], 'words': [{'text': '#', 'boundingPolygon': [{'x': 1527, 'y': 822}, {'x': 1540, 'y': 826}, {'x': 1534, 'y': 849}, {'x': 1520, 'y': 845}], 'confidence': 0.918}, {'text': 'OF', 'boundingPolygon': [{'x': 1548, 'y': 829}, {'x': 1572, 'y': 838}, {'x': 1566, 'y': 859}, {'x': 1541, 'y': 851}], 'confidence': 0.995}, {'text': 'SERVINGS', 'boundingPolygon': [{'x': 1578, 'y': 840}, {'x': 1668, 'y': 870}, {'x': 1663, 'y': 891}, {'x': 1572, 'y': 860}], 'confidence': 0.992}]}, {'text': 'APPETIZER - BREAKFAST - LUNCH - DINNER - DESSERT - SNACK', 'boundingPolygon': [{'x': 1045, 'y': 781}, {'x': 1624, 'y': 961}, {'x': 1617, 'y': 981}, {'x': 1039, 'y': 801}], 'words': [{'text': 'APPETIZER', 'boundingPolygon': [{'x': 1047, 'y': 782}, {'x': 1141, 'y': 811}, {'x': 1135, 'y': 831}, {'x': 1042, 'y': 801}], 'confidence': 0.993}, {'text': '-', 'boundingPolygon': [{'x': 1146, 'y': 813}, {'x': 1157, 'y': 816}, {'x': 1151, 'y': 836}, {'x': 1140, 'y': 833}], 'confidence': 0.993}, {'text': 'BREAKFAST', 'boundingPolygon': [{'x': 1161, 'y': 817}, {'x': 1264, 'y': 849}, {'x': 1258, 'y': 870}, {'x': 1155, 'y': 837}], 'confidence': 0.993}, {'text': '-', 'boundingPolygon': [{'x': 1268, 'y': 850}, {'x': 1280, 'y': 854}, {'x': 1273, 'y': 875}, {'x': 1262, 'y': 871}], 'confidence': 0.959}, {'text': 'LUNCH', 'boundingPolygon': [{'x': 1285, 'y': 855}, {'x': 1344, 'y': 874}, {'x': 1336, 'y': 895}, {'x': 1278, 'y': 876}], 'confidence': 0.997}, {'text': '-', 'boundingPolygon': [{'x': 1353, 'y': 876}, {'x': 1363, 'y': 879}, {'x': 1355, 'y': 901}, {'x': 1345, 'y': 897}], 'confidence': 0.993}, {'text': 'DINNER', 'boundingPolygon': [{'x': 1369, 'y': 881}, {'x': 1435, 'y': 902}, {'x': 1426, 'y': 923}, {'x': 1361, 'y': 902}], 'confidence': 0.994}, {'text': '-', 'boundingPolygon': [{'x': 1442, 'y': 904}, {'x': 1452, 'y': 907}, {'x': 1444, 'y': 928}, {'x': 1433, 'y': 925}], 'confidence': 0.992}, {'text': 'DESSERT', 'boundingPolygon': [{'x': 1458, 'y': 909}, {'x': 1538, 'y': 935}, {'x': 1529, 'y': 955}, {'x': 1450, 'y': 930}], 'confidence': 0.993}, {'text': '-', 'boundingPolygon': [{'x': 1542, 'y': 936}, {'x': 1553, 'y': 939}, {'x': 1543, 'y': 959}, {'x': 1533, 'y': 956}], 'confidence': 0.994}, {'text': 'SNACK', 'boundingPolygon': [{'x': 1560, 'y': 941}, {'x': 1619, 'y': 960}, {'x': 1609, 'y': 980}, {'x': 1550, 'y': 961}], 'confidence': 0.995}]}, {'text': 'RECIPE', 'boundingPolygon': [{'x': 1002, 'y': 824}, {'x': 1083, 'y': 851}, {'x': 1075, 'y': 877}, {'x': 995, 'y': 849}], 'words': [{'text': 'RECIPE', 'boundingPolygon': [{'x': 1007, 'y': 826}, {'x': 1072, 'y': 847}, {'x': 1066, 'y': 874}, {'x': 1001, 'y': 852}], 'confidence': 0.991}]}, {'text': 'adobo', 'boundingPolygon': [{'x': 1093, 'y': 845}, {'x': 1277, 'y': 902}, {'x': 1266, 'y': 945}, {'x': 1081, 'y': 891}], 'words': [{'text': 'adobo', 'boundingPolygon': [{'x': 1095, 'y': 845}, {'x': 1270, 'y': 899}, {'x': 1261, 'y': 943}, {'x': 1083, 'y': 890}], 'confidence': 0.799}]}, {'text': 'INGREDIENTS', 'boundingPolygon': [{'x': 1174, 'y': 932}, {'x': 1309, 'y': 973}, {'x': 1301, 'y': 997}, {'x': 1167, 'y': 955}], 'words': [{'text': 'INGREDIENTS', 'boundingPolygon': [{'x': 1180, 'y': 934}, {'x': 1305, 'y': 972}, {'x': 1298, 'y': 995}, {'x': 1174, 'y': 957}], 'confidence': 0.989}]}, {'text': 'AMOUNT', 'boundingPolygon': [{'x': 1490, 'y': 1029}, {'x': 1578, 'y': 1059}, {'x': 1571, 'y': 1078}, {'x': 1485, 'y': 1051}], 'words': [{'text': 'AMOUNT', 'boundingPolygon': [{'x': 1493, 'y': 1030}, {'x': 1573, 'y': 1057}, {'x': 1568, 'y': 1077}, {'x': 1488, 'y': 1051}], 'confidence': 0.995}]}, {'text': 'MINIMALPLANN', 'boundingPolygon': [{'x': 375, 'y': 715}, {'x': 403, 'y': 898}, {'x': 378, 'y': 902}, {'x': 352, 'y': 716}], 'words': [{'text': 'MINIMALPLANN', 'boundingPolygon': [{'x': 377, 'y': 738}, {'x': 397, 'y': 873}, {'x': 375, 'y': 876}, {'x': 354, 'y': 740}], 'confidence': 0.278}]}, {'text': 'CHOICE OF MEAT', 'boundingPolygon': [{'x': 1042, 'y': 948}, {'x': 1268, 'y': 1022}, {'x': 1259, 'y': 1055}, {'x': 1034, 'y': 982}], 'words': [{'text': 'CHOICE', 'boundingPolygon': [{'x': 1043, 'y': 948}, {'x': 1141, 'y': 982}, {'x': 1134, 'y': 1017}, {'x': 1035, 'y': 984}], 'confidence': 0.989}, {'text': 'OF', 'boundingPolygon': [{'x': 1149, 'y': 985}, {'x': 1181, 'y': 995}, {'x': 1174, 'y': 1029}, {'x': 1142, 'y': 1019}], 'confidence': 0.957}, {'text': 'MEAT', 'boundingPolygon': [{'x': 1189, 'y': 997}, {'x': 1267, 'y': 1022}, {'x': 1260, 'y': 1055}, {'x': 1181, 'y': 1032}], 'confidence': 0.989}]}, {'text': 'PORK BELLY . CHICKEN, RIBS', 'boundingPolygon': [{'x': 1042, 'y': 1006}, {'x': 1411, 'y': 1120}, {'x': 1399, 'y': 1157}, {'x': 1031, 'y': 1044}], 'words': [{'text': 'PORK', 'boundingPolygon': [{'x': 1043, 'y': 1007}, {'x': 1103, 'y': 1027}, {'x': 1093, 'y': 1063}, {'x': 1033, 'y': 1043}], 'confidence': 0.972}, {'text': 'BELLY', 'boundingPolygon': [{'x': 1110, 'y': 1029}, {'x': 1180, 'y': 1051}, {'x': 1170, 'y': 1088}, {'x': 1100, 'y': 1066}], 'confidence': 0.968}, {'text': '.', 'boundingPolygon': [{'x': 1187, 'y': 1053}, {'x': 1194, 'y': 1055}, {'x': 1184, 'y': 1093}, {'x': 1177, 'y': 1090}], 'confidence': 0.63}, {'text': 'CHICKEN,', 'boundingPolygon': [{'x': 1202, 'y': 1058}, {'x': 1329, 'y': 1096}, {'x': 1319, 'y': 1134}, {'x': 1192, 'y': 1095}], 'confidence': 0.888}, {'text': 'RIBS', 'boundingPolygon': [{'x': 1337, 'y': 1099}, {'x': 1408, 'y': 1120}, {'x': 1398, 'y': 1157}, {'x': 1327, 'y': 1136}], 'confidence': 0.969}]}, {'text': 'ONION', 'boundingPolygon': [{'x': 1005, 'y': 1053}, {'x': 1096, 'y': 1081}, {'x': 1088, 'y': 1114}, {'x': 995, 'y': 1088}], 'words': [{'text': 'ONION', 'boundingPolygon': [{'x': 1005, 'y': 1053}, {'x': 1094, 'y': 1079}, {'x': 1084, 'y': 1114}, {'x': 995, 'y': 1088}], 'confidence': 0.918}]}, {'text': 'MINIMALPLAN!', 'boundingPolygon': [{'x': 871, 'y': 1035}, {'x': 832, 'y': 1158}, {'x': 812, 'y': 1152}, {'x': 851, 'y': 1028}], 'words': [{'text': 'MINIMALPLAN!', 'boundingPolygon': [{'x': 870, 'y': 1039}, {'x': 830, 'y': 1157}, {'x': 813, 'y': 1152}, {'x': 850, 'y': 1032}], 'confidence': 0.421}]}, {'text': 'GARLIC', 'boundingPolygon': [{'x': 989, 'y': 1108}, {'x': 1096, 'y': 1142}, {'x': 1084, 'y': 1176}, {'x': 982, 'y': 1141}], 'words': [{'text': 'GARLIC', 'boundingPolygon': [{'x': 989, 'y': 1109}, {'x': 1093, 'y': 1143}, {'x': 1081, 'y': 1175}, {'x': 982, 'y': 1143}], 'confidence': 0.895}]}, {'text': 'BLACK PEPPER', 'boundingPolygon': [{'x': 970, 'y': 1156}, {'x': 1189, 'y': 1227}, {'x': 1177, 'y': 1264}, {'x': 961, 'y': 1191}], 'words': [{'text': 'BLACK', 'boundingPolygon': [{'x': 971, 'y': 1157}, {'x': 1064, 'y': 1187}, {'x': 1054, 'y': 1222}, {'x': 963, 'y': 1194}], 'confidence': 0.894}, {'text': 'PEPPER', 'boundingPolygon': [{'x': 1072, 'y': 1189}, {'x': 1183, 'y': 1227}, {'x': 1171, 'y': 1262}, {'x': 1062, 'y': 1224}], 'confidence': 0.93}]}, {'text': 'MSG', 'boundingPolygon': [{'x': 954, 'y': 1210}, {'x': 1027, 'y': 1232}, {'x': 1019, 'y': 1264}, {'x': 946, 'y': 1245}], 'words': [{'text': 'MSG', 'boundingPolygon': [{'x': 959, 'y': 1211}, {'x': 1025, 'y': 1230}, {'x': 1016, 'y': 1264}, {'x': 949, 'y': 1245}], 'confidence': 0.918}]}, {'text': 'BEEF CUBE', 'boundingPolygon': [{'x': 944, 'y': 1262}, {'x': 1090, 'y': 1310}, {'x': 1082, 'y': 1346}, {'x': 935, 'y': 1298}], 'words': [{'text': 'BEEF', 'boundingPolygon': [{'x': 945, 'y': 1263}, {'x': 1012, 'y': 1284}, {'x': 1005, 'y': 1321}, {'x': 936, 'y': 1300}], 'confidence': 0.987}, {'text': 'CUBE', 'boundingPolygon': [{'x': 1021, 'y': 1287}, {'x': 1090, 'y': 1309}, {'x': 1084, 'y': 1346}, {'x': 1013, 'y': 1324}], 'confidence': 0.918}]}, {'text': 'SOY SAUCE', 'boundingPolygon': [{'x': 927, 'y': 1312}, {'x': 1078, 'y': 1363}, {'x': 1068, 'y': 1400}, {'x': 920, 'y': 1350}], 'words': [{'text': 'SOY', 'boundingPolygon': [{'x': 927, 'y': 1313}, {'x': 979, 'y': 1332}, {'x': 972, 'y': 1371}, {'x': 920, 'y': 1352}], 'confidence': 0.495}, {'text': 'SAUCE', 'boundingPolygon': [{'x': 986, 'y': 1334}, {'x': 1074, 'y': 1361}, {'x': 1066, 'y': 1399}, {'x': 979, 'y': 1373}], 'confidence': 0.727}]}, {'text': 'VINEGAR', 'boundingPolygon': [{'x': 915, 'y': 1368}, {'x': 1048, 'y': 1417}, {'x': 1040, 'y': 1450}, {'x': 906, 'y': 1404}], 'words': [{'text': 'VINEGAR', 'boundingPolygon': [{'x': 915, 'y': 1368}, {'x': 1046, 'y': 1415}, {'x': 1041, 'y': 1449}, {'x': 907, 'y': 1407}], 'confidence': 0.924}]}, {'text': '* EGGS , POTATOES', 'boundingPolygon': [{'x': 890, 'y': 1416}, {'x': 1166, 'y': 1506}, {'x': 1152, 'y': 1543}, {'x': 882, 'y': 1457}], 'words': [{'text': '*', 'boundingPolygon': [{'x': 892, 'y': 1417}, {'x': 904, 'y': 1421}, {'x': 896, 'y': 1462}, {'x': 884, 'y': 1457}], 'confidence': 0.89}, {'text': 'EGGS', 'boundingPolygon': [{'x': 912, 'y': 1424}, {'x': 988, 'y': 1452}, {'x': 979, 'y': 1491}, {'x': 903, 'y': 1464}], 'confidence': 0.933}, {'text': ',', 'boundingPolygon': [{'x': 996, 'y': 1455}, {'x': 1008, 'y': 1459}, {'x': 998, 'y': 1498}, {'x': 986, 'y': 1494}], 'confidence': 0.877}, {'text': 'POTATOES', 'boundingPolygon': [{'x': 1015, 'y': 1461}, {'x': 1165, 'y': 1506}, {'x': 1153, 'y': 1543}, {'x': 1006, 'y': 1500}], 'confidence': 0.931}]}, {'text': 'TOTAL', 'boundingPolygon': [{'x': 1173, 'y': 1747}, {'x': 1230, 'y': 1764}, {'x': 1222, 'y': 1785}, {'x': 1166, 'y': 1772}], 'words': [{'text': 'TOTAL', 'boundingPolygon': [{'x': 1172, 'y': 1747}, {'x': 1229, 'y': 1761}, {'x': 1223, 'y': 1786}, {'x': 1166, 'y': 1771}], 'confidence': 0.994}]}]}]}}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=os.getenv(\"AZURE_CV_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_API_KEY\"))\n",
    ")\n",
    "\n",
    "result = client.analyze_from_url(\n",
    "    image_url=\"https://myminimalplanner.com/cdn/shop/files/RecipeCheck.jpg?v=1746749084&width=2000\", # Binary data from your image file\n",
    "    visual_features=[VisualFeatures.READ],\n",
    "    language=\"en\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e27ac",
   "metadata": {},
   "source": [
    "## Exercise - Read text in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import namespaces\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Clear the console\n",
    "    os.system('cls' if os.name=='nt' else 'clear')\n",
    "\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        ai_endpoint = os.getenv('AZURE_CV4_ENDPOINT')\n",
    "        ai_key = os.getenv('AZURE_CV4_KEY')\n",
    "\n",
    "        # Get image\n",
    "        image_file = 'images/Lincoln.jpg'\n",
    "        if len(sys.argv) > 1:\n",
    "            image_file = sys.argv[1]\n",
    "\n",
    "\n",
    "        # Authenticate Azure AI Vision client\n",
    "        cv_client = ImageAnalysisClient(\n",
    "            endpoint=ai_endpoint,\n",
    "            credential=AzureKeyCredential(ai_key))\n",
    "\n",
    "        \n",
    "        # Read text in image\n",
    "        with open(image_file, \"rb\") as f:\n",
    "            image_data = f.read()\n",
    "        print (f\"\\nReading text in {image_file}\")\n",
    "\n",
    "        result = cv_client.analyze(\n",
    "            image_data=image_data,\n",
    "            visual_features=[VisualFeatures.READ])\n",
    "        \n",
    "\n",
    "        # Print the text\n",
    "        if result.read is not None:\n",
    "            print(\"\\nText:\")\n",
    "            \n",
    "            for line in result.read.blocks[0].lines:\n",
    "                print(f\" {line.text}\")        \n",
    "            # Annotate the text in the image\n",
    "            annotate_lines(image_file, result.read)\n",
    "\n",
    "            # Find individual words in each line\n",
    "            print (\"\\nIndividual words:\")\n",
    "            for line in result.read.blocks[0].lines:\n",
    "                for word in line.words:\n",
    "                    print(f\"  {word.text} (Confidence: {word.confidence:.2f}%)\")\n",
    "            # Annotate the words in the image\n",
    "            annotate_words(image_file, result.read)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "def annotate_lines(image_file, detected_text):\n",
    "    print(f'\\nAnnotating lines of text in image...')\n",
    "\n",
    "     # Prepare image for drawing\n",
    "    image = Image.open(image_file)\n",
    "    fig = plt.figure(figsize=(image.width/100, image.height/100))\n",
    "    plt.axis('off')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color = 'cyan'\n",
    "\n",
    "    for line in detected_text.blocks[0].lines:\n",
    "        # Draw line bounding polygon\n",
    "        r = line.bounding_polygon\n",
    "        rectangle = ((r[0].x, r[0].y),(r[1].x, r[1].y),(r[2].x, r[2].y),(r[3].x, r[3].y))\n",
    "        draw.polygon(rectangle, outline=color, width=3)\n",
    "\n",
    "    # Save image\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout(pad=0)\n",
    "    textfile = 'lines.jpg'\n",
    "    fig.savefig(textfile)\n",
    "    print('  Results saved in', textfile)\n",
    "    \n",
    "def annotate_words(image_file, detected_text):\n",
    "    print(f'\\nAnnotating individual words in image...')\n",
    "\n",
    "     # Prepare image for drawing\n",
    "    image = Image.open(image_file)\n",
    "    fig = plt.figure(figsize=(image.width/100, image.height/100))\n",
    "    plt.axis('off')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color = 'cyan'\n",
    "\n",
    "    for line in detected_text.blocks[0].lines:\n",
    "        for word in line.words:\n",
    "            # Draw word bounding polygon\n",
    "            r = word.bounding_polygon\n",
    "            rectangle = ((r[0].x, r[0].y),(r[1].x, r[1].y),(r[2].x, r[2].y),(r[3].x, r[3].y))\n",
    "            draw.polygon(rectangle, outline=color, width=3)\n",
    "\n",
    "    # Save image\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout(pad=0)\n",
    "    textfile = 'words.jpg'\n",
    "    fig.savefig(textfile)\n",
    "    print('  Results saved in', textfile)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c0a7d1",
   "metadata": {},
   "source": [
    "# Face recognition\n",
    "\n",
    "The Face service provides functionality that you can use for:\n",
    "\n",
    "- Face detection - for each detected face, the results include an ID that identifies the face and the bounding box coordinates indicating its location in the image.\n",
    "- Face attribute analysis - you can return a wide range of facial attributes, including:\n",
    "- Head pose (pitch, roll, and yaw orientation in 3D space)\n",
    "    - Glasses (No glasses, Reading glasses, Sunglasses, or Swimming Goggles)\n",
    "    - Mask (the presence of a face mask)\n",
    "    - Blur (low, medium, or high)\n",
    "    - Exposure (under exposure, good exposure, or over exposure)\n",
    "    - Noise (visual noise in the image)\n",
    "    - Occlusion (objects obscuring the face)\n",
    "    - Accessories (glasses, headwear, mask)\n",
    "    - QualityForRecognition (low, medium, or high)\n",
    "- Facial landmark location - coordinates for key landmarks in relation to facial features (for example, eye corners, pupils, tip of nose, and so on)\n",
    "- Face comparison - you can compare faces across multiple images for similarity (to find individuals with similar facial features) and verification (to determine that a face in one image is the same person as a face in another image)\n",
    "- Facial recognition - you can train a model with a collection of faces belonging to specific individuals, and use the model to identify those people in new images.\n",
    "- Facial liveness - liveness can be used to determine if the input video is a real stream or a fake to prevent bad-intentioned individuals from spoofing a facial recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feaabec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.vision.face import FaceClient\n",
    "from azure.ai.vision.face.models import *\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "face_client = FaceClient(\n",
    "    endpoint=os.getenv(\"AZURE_FACE_END\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_FACE_KEY\")))\n",
    "\n",
    "features = [FaceAttributeTypeDetection01.HEAD_POSE,\n",
    "            FaceAttributeTypeDetection01.OCCLUSION,\n",
    "            FaceAttributeTypeDetection01.ACCESSORIES]\n",
    "\n",
    "# Use client to detect faces in an image\n",
    "with open(\"mslearn-ai-vision/Labfiles/analyze-images/python/image-analysis/images/person.jpg\", mode=\"rb\") as image_data:\n",
    "    detected_faces = face_client.detect(\n",
    "        image_content=image_data.read(),\n",
    "        detection_model=FaceDetectionModel.DETECTION01,\n",
    "        recognition_model=FaceRecognitionModel.RECOGNITION01,\n",
    "        return_face_id=True,\n",
    "        return_face_attributes=features,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf04241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import namespaces\n",
    "from azure.ai.vision.face import FaceClient\n",
    "from azure.ai.vision.face.models import FaceDetectionModel, FaceRecognitionModel, FaceAttributeTypeDetection01\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Clear the console\n",
    "    os.system('cls' if os.name=='nt' else 'clear')\n",
    "\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        cog_endpoint = os.getenv('AZURE_FACE_END')\n",
    "        cog_key = os.getenv('AZURE_FACE_KEY')\n",
    "\n",
    "        # Get image\n",
    "        image_file = 'images/face1.jpg'\n",
    "        if len(sys.argv) > 1:\n",
    "            image_file = sys.argv[1]\n",
    "\n",
    "\n",
    "        # Authenticate Face client\n",
    "        face_client = FaceClient(\n",
    "            endpoint=cog_endpoint,\n",
    "            credential=AzureKeyCredential(cog_key))\n",
    "\n",
    "\n",
    "\n",
    "        # Specify facial features to be retrieved\n",
    "        features = [FaceAttributeTypeDetection01.HEAD_POSE,\n",
    "                    FaceAttributeTypeDetection01.OCCLUSION,\n",
    "                    FaceAttributeTypeDetection01.ACCESSORIES]\n",
    "\n",
    "\n",
    "        ## Get faces\n",
    "        with open(image_file, mode=\"rb\") as image_data:\n",
    "            detected_faces = face_client.detect(\n",
    "                image_content=image_data.read(),\n",
    "                detection_model=FaceDetectionModel.DETECTION01,\n",
    "                recognition_model=FaceRecognitionModel.RECOGNITION01,\n",
    "                return_face_id=False,\n",
    "                return_face_attributes=features,\n",
    "            )\n",
    "\n",
    "        face_count = 0\n",
    "        if len(detected_faces) > 0:\n",
    "            print(len(detected_faces), 'faces detected.')\n",
    "            for face in detected_faces:\n",
    "            \n",
    "                # Get face properties\n",
    "                face_count += 1\n",
    "                print('\\nFace number {}'.format(face_count))\n",
    "                print(' - Head Pose (Yaw): {}'.format(face.face_attributes.head_pose.yaw))\n",
    "                print(' - Head Pose (Pitch): {}'.format(face.face_attributes.head_pose.pitch))\n",
    "                print(' - Head Pose (Roll): {}'.format(face.face_attributes.head_pose.roll))\n",
    "                print(' - Forehead occluded?: {}'.format(face.face_attributes.occlusion[\"foreheadOccluded\"]))\n",
    "                print(' - Eye occluded?: {}'.format(face.face_attributes.occlusion[\"eyeOccluded\"]))\n",
    "                print(' - Mouth occluded?: {}'.format(face.face_attributes.occlusion[\"mouthOccluded\"]))\n",
    "                print(' - Accessories:')\n",
    "                for accessory in face.face_attributes.accessories:\n",
    "                    print('   - {}'.format(accessory.type))\n",
    "                # Annotate faces in the image\n",
    "                annotate_faces(image_file, detected_faces)\n",
    " \n",
    " \n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "def annotate_faces(image_file, detected_faces):\n",
    "    print('\\nAnnotating faces in image...')\n",
    "\n",
    "    # Prepare image for drawing\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    plt.axis('off')\n",
    "    image = Image.open(image_file)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color = 'lightgreen'\n",
    "\n",
    "    # Annotate each face in the image\n",
    "    face_count = 0\n",
    "    for face in detected_faces:\n",
    "        face_count += 1\n",
    "        r = face.face_rectangle\n",
    "        bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.rectangle(bounding_box, outline=color, width=5)\n",
    "        annotation = 'Face number {}'.format(face_count)\n",
    "        plt.annotate(annotation,(r.left, r.top), backgroundcolor=color)\n",
    "    \n",
    "    # Save annotated image\n",
    "    plt.imshow(image)\n",
    "    outputfile = 'detected_faces.jpg'\n",
    "    fig.savefig(outputfile)\n",
    "    print(f'  Results saved in {outputfile}\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799e29b",
   "metadata": {},
   "source": [
    "# Clasify img\n",
    "\n",
    "## Azure AI Custom Vision\n",
    "\n",
    "The Azure AI Custom Vision service enables you to build your own computer vision models for image classification or object detection.\n",
    "\n",
    "To use the Custom Vision service to create a solution, you need two Custom Vision resources in your Azure subscription:\n",
    "\n",
    "- An Azure AI Custom Vision training resource - used to train a custom model based on your own training images.\n",
    "- An Azure AI Custom Vision prediction resource - used to generate predictions from new images based on your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f58917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msrest.authentication import ApiKeyCredentials\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "\n",
    "\n",
    " # Authenticate a client for the prediction API\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": \"<YOUR_PREDICTION_RESOURCE_KEY>\"})\n",
    "prediction_client = CustomVisionPredictionClient(endpoint=\"<YOUR_PREDICTION_RESOURCE_ENDPOINT>\",\n",
    "                                                 credentials=credentials)\n",
    "\n",
    "# Get classification predictions for an image\n",
    "image_data = open(\"/Users/A200315276/Desktop/AI_Engineer/mslearn-ai-vision/Labfiles/image-classification/test-images/IMG_TEST_1.jpg\", \"rb\").read()\n",
    "results = prediction_client.classify_image(\"<YOUR_PROJECT_ID>\",\n",
    "                                           \"<YOUR_PUBLISHED_MODEL_NAME>\",\n",
    "                                           image_data)\n",
    "\n",
    "# Process predictions\n",
    "for prediction in results.predictions:\n",
    "    if prediction.probability > 0.5:\n",
    "        print(image, ': {} ({:.0%})'.format(prediction.tag_name, prediction.probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48154b",
   "metadata": {},
   "source": [
    "## Exercise - Classify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e318e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2JUploading images...\n",
      "orange\n",
      "apple\n",
      "banana\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Training ...\n",
      "Completed ...\n",
      "Model trained!\n"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\n",
    "from azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "import time\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    from dotenv import load_dotenv\n",
    "    global training_client\n",
    "    global custom_vision_project\n",
    "\n",
    "    # Clear the console\n",
    "    os.system('cls' if os.name=='nt' else 'clear')\n",
    "\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        training_endpoint = os.getenv('AZURE_CLAS_END')\n",
    "        training_key = os.getenv('AZURE_CLAS_KEY')\n",
    "        project_id = os.getenv('AZURE_CLAS_PID')\n",
    "\n",
    "        # Authenticate a client for the training API\n",
    "        credentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\n",
    "        training_client = CustomVisionTrainingClient(training_endpoint, credentials)\n",
    "\n",
    "        # Get the Custom Vision project\n",
    "        custom_vision_project = training_client.get_project(project_id)\n",
    "\n",
    "        # Upload and tag images\n",
    "        Upload_Images('mslearn-ai-vision/Labfiles/image-classification/python/train-classifier/more-training-images')\n",
    "\n",
    "        # Train the model\n",
    "        Train_Model()\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "def Upload_Images(folder):\n",
    "    print(\"Uploading images...\")\n",
    "    tags = training_client.get_tags(custom_vision_project.id)\n",
    "    for tag in tags:\n",
    "        print(tag.name)\n",
    "        for image in os.listdir(os.path.join(folder,tag.name)):\n",
    "            image_data = open(os.path.join(folder,tag.name,image), \"rb\").read()\n",
    "            training_client.create_images_from_data(custom_vision_project.id, image_data, [tag.id])\n",
    "\n",
    "def Train_Model():\n",
    "    print(\"Training ...\")\n",
    "    iteration = training_client.train_project(custom_vision_project.id)\n",
    "    while (iteration.status != \"Completed\"):\n",
    "        iteration = training_client.get_iteration(custom_vision_project.id, iteration.id)\n",
    "        print (iteration.status, '...')\n",
    "        time.sleep(5)\n",
    "    print (\"Model trained!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e75415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2JIMG_TEST_3.jpg : orange (100%)\n",
      "IMG_TEST_2.jpg : banana (100%)\n",
      "IMG_TEST_1.jpg : apple (100%)\n"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    # Clear the console\n",
    "    os.system('cls' if os.name=='nt' else 'clear')\n",
    "\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        prediction_endpoint = os.getenv('AZURE_CLAS_PRED_END')\n",
    "        prediction_key = os.getenv('AZURE_CLAS_PRED_KEY')\n",
    "        project_id = os.getenv('AZURE_CLAS_PID')\n",
    "        model_name = os.getenv('AZURE_CLAS_PRED_MODEL')\n",
    "\n",
    "        # Authenticate a client for the prediction API\n",
    "        credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n",
    "        prediction_client = CustomVisionPredictionClient(endpoint=prediction_endpoint, credentials=credentials)\n",
    "\n",
    "        # Classify test images\n",
    "        for image in os.listdir('mslearn-ai-vision/Labfiles/image-classification/python/test-classifier/test-images'):\n",
    "            image_data = open(os.path.join('mslearn-ai-vision/Labfiles/image-classification/python/test-classifier/test-images',image), \"rb\").read()\n",
    "            results = prediction_client.classify_image(project_id, model_name, image_data)\n",
    "\n",
    "            # Loop over each label prediction and print any with probability > 50%\n",
    "            for prediction in results.predictions:\n",
    "                if prediction.probability > 0.5:\n",
    "                    print(image, ': {} ({:.0%})'.format(prediction.tag_name, prediction.probability))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
